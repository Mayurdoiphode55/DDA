import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Set style for better visualizations
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

print("=" * 60)
print("UBER FARE PREDICTION - ML PIPELINE")
print("=" * 60)

# ============================================================================
# 1. LOAD AND EXPLORE DATASET
# ============================================================================
print("\n[1] Loading Dataset...")
# Load the dataset (update path as needed)
df = pd.read_csv('uber.csv')

print(f"\nDataset Shape: {df.shape}")
print(f"\nFirst 5 rows:")
print(df.head())
print(f"\nDataset Info:")
print(df.info())
print(f"\nBasic Statistics:")
print(df.describe())
print(f"\nMissing Values:")
print(df.isnull().sum())

# ============================================================================
# 2. DATA PREPROCESSING
# ============================================================================
print("\n" + "=" * 60)
print("[2] Data Preprocessing...")
print("=" * 60)

# Remove any rows with missing values
df = df.dropna()
print(f"\nShape after removing missing values: {df.shape}")

# Convert pickup_datetime to datetime
df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])

# Extract datetime features
df['year'] = df['pickup_datetime'].dt.year
df['month'] = df['pickup_datetime'].dt.month
df['day'] = df['pickup_datetime'].dt.day
df['hour'] = df['pickup_datetime'].dt.hour
df['day_of_week'] = df['pickup_datetime'].dt.dayofweek

# Calculate distance using Haversine formula
def haversine_distance(lat1, lon1, lat2, lon2):
    """Calculate the great circle distance between two points on earth"""
    R = 6371  # Radius of earth in kilometers
    
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    distance = R * c
    
    return distance

df['distance'] = haversine_distance(
    df['pickup_latitude'], df['pickup_longitude'],
    df['dropoff_latitude'], df['dropoff_longitude']
)

print("\nNew features created:")
print("- year, month, day, hour, day_of_week")
print("- distance (calculated using Haversine formula)")

# ============================================================================
# 3. IDENTIFY AND REMOVE OUTLIERS
# ============================================================================
print("\n" + "=" * 60)
print("[3] Identifying and Removing Outliers...")
print("=" * 60)

# Remove invalid fare amounts
initial_count = len(df)
df = df[(df['fare_amount'] > 0) & (df['fare_amount'] <= 500)]
print(f"Removed {initial_count - len(df)} rows with invalid fare amounts")

# Remove invalid passenger counts
df = df[(df['passenger_count'] > 0) & (df['passenger_count'] <= 8)]

# Remove invalid coordinates (NYC boundaries approximately)
df = df[(df['pickup_latitude'] >= 40.5) & (df['pickup_latitude'] <= 41.0)]
df = df[(df['pickup_longitude'] >= -74.3) & (df['pickup_longitude'] <= -73.7)]
df = df[(df['dropoff_latitude'] >= 40.5) & (df['dropoff_latitude'] <= 41.0)]
df = df[(df['dropoff_longitude'] >= -74.3) & (df['dropoff_longitude'] <= -73.7)]

# Remove trips with distance = 0 or unreasonably long
df = df[(df['distance'] > 0) & (df['distance'] <= 100)]

print(f"\nFinal dataset shape after outlier removal: {df.shape}")

# Visualize outliers using boxplots
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
axes[0].boxplot(df['fare_amount'])
axes[0].set_title('Fare Amount Distribution')
axes[0].set_ylabel('USD')

axes[1].boxplot(df['distance'])
axes[1].set_title('Distance Distribution')
axes[1].set_ylabel('Kilometers')

axes[2].boxplot(df['passenger_count'])
axes[2].set_title('Passenger Count Distribution')
axes[2].set_ylabel('Count')

plt.tight_layout()
plt.savefig('outliers_analysis.png', dpi=300, bbox_inches='tight')
print("\nOutlier analysis plot saved as 'outliers_analysis.png'")

# ============================================================================
# 4. CORRELATION ANALYSIS
# ============================================================================
print("\n" + "=" * 60)
print("[4] Correlation Analysis...")
print("=" * 60)

# Select numeric features for correlation
numeric_features = ['fare_amount', 'passenger_count', 'distance', 
                   'year', 'month', 'day', 'hour', 'day_of_week']

correlation_matrix = df[numeric_features].corr()
print("\nCorrelation Matrix:")
print(correlation_matrix['fare_amount'].sort_values(ascending=False))

# Visualize correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            fmt='.2f', square=True, linewidths=1)
plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')
print("\nCorrelation heatmap saved as 'correlation_heatmap.png'")

# ============================================================================
# 5. PREPARE DATA FOR MODELING
# ============================================================================
print("\n" + "=" * 60)
print("[5] Preparing Data for Modeling...")
print("=" * 60)

# Select features for modeling
feature_cols = ['passenger_count', 'distance', 'year', 'month', 
                'day', 'hour', 'day_of_week']

X = df[feature_cols]
y = df['fare_amount']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\nTraining set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")
print(f"\nFeatures used: {feature_cols}")

# ============================================================================
# 6. LINEAR REGRESSION MODEL
# ============================================================================
print("\n" + "=" * 60)
print("[6] Linear Regression Model...")
print("=" * 60)

# Train Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
print("Linear Regression model trained successfully!")

# Make predictions
y_pred_lr_train = lr_model.predict(X_train)
y_pred_lr_test = lr_model.predict(X_test)

# Evaluate Linear Regression
lr_r2_train = r2_score(y_train, y_pred_lr_train)
lr_r2_test = r2_score(y_test, y_pred_lr_test)
lr_rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_lr_train))
lr_rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_lr_test))
lr_mae_train = mean_absolute_error(y_train, y_pred_lr_train)
lr_mae_test = mean_absolute_error(y_test, y_pred_lr_test)

print("\n--- Linear Regression Results ---")
print(f"Training RÂ² Score: {lr_r2_train:.4f}")
print(f"Testing RÂ² Score: {lr_r2_test:.4f}")
print(f"Training RMSE: ${lr_rmse_train:.2f}")
print(f"Testing RMSE: ${lr_rmse_test:.2f}")
print(f"Training MAE: ${lr_mae_train:.2f}")
print(f"Testing MAE: ${lr_mae_test:.2f}")

# Feature importance for Linear Regression
print("\nFeature Coefficients:")
for feature, coef in zip(feature_cols, lr_model.coef_):
    print(f"  {feature}: {coef:.4f}")

# ============================================================================
# 7. RANDOM FOREST REGRESSION MODEL
# ============================================================================
print("\n" + "=" * 60)
print("[7] Random Forest Regression Model...")
print("=" * 60)

# Train Random Forest
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
print("Training Random Forest model (this may take a moment)...")
rf_model.fit(X_train, y_train)
print("Random Forest model trained successfully!")

# Make predictions
y_pred_rf_train = rf_model.predict(X_train)
y_pred_rf_test = rf_model.predict(X_test)

# Evaluate Random Forest
rf_r2_train = r2_score(y_train, y_pred_rf_train)
rf_r2_test = r2_score(y_test, y_pred_rf_test)
rf_rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_rf_train))
rf_rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_rf_test))
rf_mae_train = mean_absolute_error(y_train, y_pred_rf_train)
rf_mae_test = mean_absolute_error(y_test, y_pred_rf_test)

print("\n--- Random Forest Results ---")
print(f"Training RÂ² Score: {rf_r2_train:.4f}")
print(f"Testing RÂ² Score: {rf_r2_test:.4f}")
print(f"Training RMSE: ${rf_rmse_train:.2f}")
print(f"Testing RMSE: ${rf_rmse_test:.2f}")
print(f"Training MAE: ${rf_mae_train:.2f}")
print(f"Testing MAE: ${rf_mae_test:.2f}")

# Feature importance for Random Forest
print("\nFeature Importances:")
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)
print(feature_importance.to_string(index=False))

# ============================================================================
# 8. MODEL COMPARISON
# ============================================================================
print("\n" + "=" * 60)
print("[8] Model Comparison and Evaluation")
print("=" * 60)

# Create comparison dataframe
comparison_df = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest'],
    'Train RÂ²': [lr_r2_train, rf_r2_train],
    'Test RÂ²': [lr_r2_test, rf_r2_test],
    'Train RMSE': [lr_rmse_train, rf_rmse_train],
    'Test RMSE': [lr_rmse_test, rf_rmse_test],
    'Train MAE': [lr_mae_train, rf_mae_train],
    'Test MAE': [lr_mae_test, rf_mae_test]
})

print("\n" + comparison_df.to_string(index=False))

# Visualize model comparison
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# RÂ² Score comparison
metrics = ['Train RÂ²', 'Test RÂ²']
lr_scores = [lr_r2_train, lr_r2_test]
rf_scores = [rf_r2_train, rf_r2_test]
x = np.arange(len(metrics))
width = 0.35

axes[0].bar(x - width/2, lr_scores, width, label='Linear Regression', color='skyblue')
axes[0].bar(x + width/2, rf_scores, width, label='Random Forest', color='lightcoral')
axes[0].set_ylabel('RÂ² Score')
axes[0].set_title('RÂ² Score Comparison')
axes[0].set_xticks(x)
axes[0].set_xticklabels(metrics)
axes[0].legend()
axes[0].grid(axis='y', alpha=0.3)

# RMSE comparison
metrics = ['Train RMSE', 'Test RMSE']
lr_rmse = [lr_rmse_train, lr_rmse_test]
rf_rmse = [rf_rmse_train, rf_rmse_test]

axes[1].bar(x - width/2, lr_rmse, width, label='Linear Regression', color='skyblue')
axes[1].bar(x + width/2, rf_rmse, width, label='Random Forest', color='lightcoral')
axes[1].set_ylabel('RMSE ($)')
axes[1].set_title('RMSE Comparison')
axes[1].set_xticks(x)
axes[1].set_xticklabels(metrics)
axes[1].legend()
axes[1].grid(axis='y', alpha=0.3)

# MAE comparison
metrics = ['Train MAE', 'Test MAE']
lr_mae = [lr_mae_train, lr_mae_test]
rf_mae = [rf_mae_train, rf_mae_test]

axes[2].bar(x - width/2, lr_mae, width, label='Linear Regression', color='skyblue')
axes[2].bar(x + width/2, rf_mae, width, label='Random Forest', color='lightcoral')
axes[2].set_ylabel('MAE ($)')
axes[2].set_title('MAE Comparison')
axes[2].set_xticks(x)
axes[2].set_xticklabels(metrics)
axes[2].legend()
axes[2].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
print("\nModel comparison plot saved as 'model_comparison.png'")

# Prediction vs Actual plots
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Linear Regression
axes[0].scatter(y_test, y_pred_lr_test, alpha=0.5, s=10)
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
             'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('Actual Fare ($)')
axes[0].set_ylabel('Predicted Fare ($)')
axes[0].set_title(f'Linear Regression\nRÂ² = {lr_r2_test:.4f}, RMSE = ${lr_rmse_test:.2f}')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Random Forest
axes[1].scatter(y_test, y_pred_rf_test, alpha=0.5, s=10, color='coral')
axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
             'r--', lw=2, label='Perfect Prediction')
axes[1].set_xlabel('Actual Fare ($)')
axes[1].set_ylabel('Predicted Fare ($)')
axes[1].set_title(f'Random Forest\nRÂ² = {rf_r2_test:.4f}, RMSE = ${rf_rmse_test:.2f}')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('predictions_vs_actual.png', dpi=300, bbox_inches='tight')
print("Prediction plots saved as 'predictions_vs_actual.png'")

# ============================================================================
# 9. CONCLUSION
# ============================================================================
print("\n" + "=" * 60)
print("[9] Conclusion")
print("=" * 60)

if rf_r2_test > lr_r2_test:
    winner = "Random Forest"
    improvement = ((rf_r2_test - lr_r2_test) / lr_r2_test) * 100
else:
    winner = "Linear Regression"
    improvement = ((lr_r2_test - rf_r2_test) / rf_r2_test) * 100

print(f"\nğŸ† Best Model: {winner}")
print(f"ğŸ“ˆ Improvement: {improvement:.2f}%")
print(f"\nKey Insights:")
print(f"- Distance is the most important predictor of fare amount")
print(f"- {winner} provides better generalization on test data")
print(f"- The models explain approximately {max(rf_r2_test, lr_r2_test)*100:.1f}% of variance in fare prices")

print("\n" + "=" * 60)
print("ANALYSIS COMPLETE!")
print("=" * 60)